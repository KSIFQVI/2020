{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from datetime import datetime \n",
    "from dateutil.relativedelta import *\n",
    "from tqdm import tqdm_notebook\n",
    "from time import sleep\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distress_detector(data_path, backtest_flag=False, **kwargs):\n",
    "    # input example('Data/msf_with_index_ret.pck', False, backtest_start_date = '2011-01-01',\n",
    "    #               backtest_end_date = '2018-01,01',exch_type = 'KOSDAQ', target_date = '2018-01-01')\n",
    "\n",
    "    # Parameter_Setting\n",
    "    exch_type = kwargs.get('exch_type', None)\n",
    "    backtest_start_date = kwargs.get('backtest_start_date', None)\n",
    "    backtest_end_date = kwargs.get('backtest_end_date', None)\n",
    "    target_date = kwargs.get('target_date', None)\n",
    "    if backtest_flag == True and (backtest_start_date == None or backtest_end_date == None \n",
    "                                  or exch_type == None):\n",
    "        print('Please include exch_type, backtest_start_date,and backtest_end_date')\n",
    "        return\n",
    "    elif backtest_flag == False and (target_date == None):\n",
    "        print('Please include target_date')\n",
    "        return\n",
    "\n",
    "    # Read data based on file types\n",
    "    def read_file():\n",
    "        temp = data_path[-3:]\n",
    "        if temp.lower() == 'csv':\n",
    "            data = pd.read_csv(data_path, parse_dates=['mdate'])\n",
    "        elif temp.lower() == 'pck':\n",
    "            data = pd.read_pickle(data_path)\n",
    "        else:\n",
    "            print('Unsupported Data Type')\n",
    "        return data\n",
    "\n",
    "    # Factor Preparation with Date Slicing\n",
    "    def factor_prep(data):\n",
    "        data = data.set_index('mdate')\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "\n",
    "        # Sorting by Exchange\n",
    "        data['index_ret'] = 0\n",
    "        if exch_type.upper() == 'KSE' or exch_type.upper() == 'KOSPI':\n",
    "            data = data[data['exchcd'] == 'KSE']\n",
    "            data['index_ret'] = data.loc[:, 'vwret_kospi']\n",
    "        elif exch_type.upper() == 'KOSDAQ':\n",
    "            data = data[data['exchcd'] == 'KOSDAQ']\n",
    "            data['index_ret'] = data.loc[:, 'vwret_kosdaq']\n",
    "        else:\n",
    "            print('Error in Exchange Name')\n",
    "\n",
    "        # Adding log return values to data\n",
    "        data['log_return'] = np.log(1 + data['ret'])\n",
    "        \n",
    "        # Adding sum of me per exchange type\n",
    "        data['sum_of_me_each_exchcd'] = data.groupby(['exchcd', 'mdate'])['me'].transform('sum')\n",
    "\n",
    "        # \"Trading Halt Reason Filter => trading_halt_LEAD_1\n",
    "        trading_halt_reason_key = keyword = ['상장폐지', '불성실', '회생', '실질심사', '파산', '상장적격',\n",
    "                                             '자본잠식', '감사의견거절', '관리종목', '미제출']\n",
    "        data['trading_halt_during_month_LEAD_1'] = 0\n",
    "        temp = data.groupby('code').shift(-1)['trading_halt_reason']\n",
    "        temp = temp.str.contains(pat='|'.join(trading_halt_reason_key), regex=True, na=False)\n",
    "        data.loc[temp, 'trading_halt_during_month_LEAD_1'] = 1\n",
    "\n",
    "        # Consolidated Factor for managed, halted, delisted   ==>   DSTR\n",
    "        cond_1 = data['managed_during_month_LEAD_1'] == 0\n",
    "        cond_2 = data['trading_halt_during_month_LEAD_1'] == 1\n",
    "        cond_3 = data['delisted_LEAD_1'] == 1\n",
    "        data['DSTR'] = 0\n",
    "        DSTR = data['DSTR']\n",
    "        DSTR[cond_1 | cond_2 | cond_3] = 1  # if distressed DSTR = 1 otherwise 0\n",
    "\n",
    "        # adjusting indicators\n",
    "        me = data['me'] * 1000000\n",
    "        ltq = data['ltq'] * 1000\n",
    "        niq = data['niq'] * 1000\n",
    "        atq = data['atq'] * 1000\n",
    "        sum_of_me_each_exchcd = data['sum_of_me_each_exchcd'] * 1000000\n",
    "        prccadj = data['prccadj_hypo']\n",
    "        log_return = data['log_return']\n",
    "        vol_3m = data['vol_3m']\n",
    "        index_ret = data['index_ret']\n",
    "\n",
    "        # Adding columns with new factors\n",
    "        data['NIMTA'] = niq / (me + ltq)\n",
    "        data['TLMTA'] = ltq / (me + ltq)\n",
    "        data['EXRET'] = log_return - index_ret\n",
    "        data['RSIZE'] = np.log(me / sum_of_me_each_exchcd)\n",
    "        data['SIGMA'] = vol_3m\n",
    "        data['MB'] = me / (atq - ltq)\n",
    "        data['LPRICE'] = np.log(prccadj)\n",
    "\n",
    "        # Drop Infinite or NA values in Data\n",
    "        temp = ['NIMTA', 'TLMTA', 'EXRET', 'RSIZE', 'SIGMA', 'MB', 'LPRICE']\n",
    "        data = data.replace([np.inf, -np.inf], np.nan).dropna(subset=temp)\n",
    "\n",
    "        return data\n",
    "\n",
    "    # Sampling with conditions\n",
    "    def sample_wcond(data):\n",
    "\n",
    "        # TDSTR and FDSTR setting\n",
    "        global TDSTR\n",
    "        TDSTR = data[data['DSTR'] == 1]\n",
    "        TDSTR = TDSTR.sort_index()\n",
    "        TDSTR.index = pd.to_datetime(TDSTR.index)\n",
    "        FDSTR = data.loc[~data['code'].isin(TDSTR['code'])]\n",
    "        FDSTR = FDSTR.sort_index()\n",
    "        FDSTR.index = pd.to_datetime(FDSTR.index)\n",
    "\n",
    "        # median number of overlapped distressed firms\n",
    "        num_dup = int(TDSTR[TDSTR.duplicated('code', keep=False)].groupby('code').size().median())\n",
    "\n",
    "        # Sampling Function\n",
    "        def sample(TDSTR, FDSTR):\n",
    "            func = lambda x: pd.DataFrame.sample(x, n=num_dup, random_state=seed_num)\n",
    "            FDSTR['mdate'] = FDSTR.index\n",
    "            a = FDSTR.groupby('code').apply(func)\n",
    "            result = a.set_index('mdate').sort_index()\n",
    "            result.index = pd.to_datetime(result.index)\n",
    "            return result\n",
    "\n",
    "        # firm that are overlapped less than num_dup times\n",
    "        temp = FDSTR[FDSTR.duplicated('code', keep=False)]['code'].value_counts() < num_dup\n",
    "        temp1 = FDSTR[FDSTR['code'].isin(temp[temp].index)]\n",
    "        # unique firms\n",
    "        temp2 = FDSTR[~FDSTR.duplicated('code', keep=False)]\n",
    "        # firms that are overlapped equal or more than num_dup times\n",
    "        temp = FDSTR[FDSTR.duplicated('code', keep=False)]['code'].value_counts() >= num_dup\n",
    "        temp3 = sample(TDSTR, FDSTR[FDSTR['code'].isin(temp[temp].index)])\n",
    "        temp = pd.concat([temp1, temp2])\n",
    "        temp = pd.concat([temp, temp3])\n",
    "        return temp\n",
    "\n",
    "    # Dynamic Logistic Regression and Bagging\n",
    "    def dynamic_logistic(data, prediction_data, num_iter, solver_type, scaler_type):\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        import pickle\n",
    "        if scaler_type.lower() == 'minmaxscaler':\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaler_type.lower() == 'standardscaler':\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type.lower() == 'maxabsscaler':\n",
    "            from sklearn.preprocessing import MaxAbsScaler\n",
    "            scaler = MaxAbsScaler()\n",
    "        elif scaler_type.lower() == 'robustscaler':\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            scaler = RobustScaler()\n",
    "\n",
    "        # Iterations for Bagging Process Begin Here\n",
    "        model = {}\n",
    "        for model_num in range(num_iter):\n",
    "            # Set seed number\n",
    "            global seed_num\n",
    "            seed_num = model_num + 1\n",
    "\n",
    "            # Formation of complete train data\n",
    "            temp = sample_wcond(data)\n",
    "            train_set = pd.concat([TDSTR, temp])\n",
    "\n",
    "            # Model Fitting\n",
    "            # Standardization with StandardScaler\n",
    "            factors = ['NIMTA', 'TLMTA', 'EXRET', 'RSIZE', 'SIGMA', 'MB', 'LPRICE']\n",
    "            temp1 = scaler.fit_transform(train_set.loc[:, factors])\n",
    "            temp2 = train_set['DSTR']\n",
    "            clf = LogisticRegression(solver=solver_type)\n",
    "            clf.fit(temp1, temp2)\n",
    "            model[model_num] = pickle.dumps(clf)\n",
    "\n",
    "        # Define prediction_data\n",
    "        data = prediction_data\n",
    "\n",
    "        # Dropping infinite and NA values and data Standardization\n",
    "        temp = ['code', 'firmname', 'NIMTA', 'TLMTA', 'EXRET', 'RSIZE', 'SIGMA', 'MB', 'LPRICE', 'DSTR']\n",
    "        valid_set = data.replace([np.inf, -np.inf], np.nan).dropna(subset=temp)\n",
    "        valid_train = scaler.fit_transform(valid_set.loc[:, factors])\n",
    "\n",
    "        # Iterating Prediction of DSTR Probability\n",
    "        temp = []\n",
    "        for model_num in range(num_iter):\n",
    "            clf = pickle.loads(model[model_num])\n",
    "            y_pred = clf.predict_proba(valid_train)\n",
    "            temp.append(f'pred_{model_num + 1}')\n",
    "            valid_set[f'pred_{model_num + 1}'] = y_pred[:, 1]\n",
    "            valid_set['predict'] = valid_set[temp].mean(axis=1)\n",
    "\n",
    "        valid_set['REAL'] = valid_set['DSTR']\n",
    "\n",
    "        # Return values needed\n",
    "        temp = ['code', 'firmname', 'NIMTA', 'TLMTA', 'EXRET', 'RSIZE', 'SIGMA', 'MB', 'LPRICE', 'DSTR', 'predict',\n",
    "                'REAL']\n",
    "\n",
    "        return valid_set[temp]\n",
    "\n",
    "    # Validate_cutoff\n",
    "    def validate_cutoff(valid_data):\n",
    "\n",
    "        def filter_by_cutoff(value_cut, df):\n",
    "            col_name = str(value_cut)\n",
    "            df[col_name] = 0\n",
    "            df[col_name] = df['predict'].apply(lambda x: 1 if x > value_cut else 0)\n",
    "\n",
    "        def summary_score(need_data):\n",
    "\n",
    "            REAL = need_data['REAL']\n",
    "            TARGET = need_data.iloc[:, (list(need_data.columns).index('REAL') + 1):]\n",
    "            predict_summary = pd.DataFrame(columns=['precision_score', 'recall_score', 'f1_score'])\n",
    "\n",
    "            for i in TARGET.columns:\n",
    "                p_s = precision_score(REAL, TARGET[i])\n",
    "                r_s = recall_score(REAL, TARGET[i])\n",
    "                f_s = f1_score(REAL, TARGET[i])\n",
    "                temp_f = pd.DataFrame([[p_s, r_s, f_s]], columns=['precision_score', 'recall_score', 'f1_score'],\n",
    "                                      index=[i])\n",
    "\n",
    "                predict_summary = predict_summary.append(temp_f)\n",
    "            return (predict_summary)\n",
    "\n",
    "        iter_num = [i / 100 for i in range(40, 100, 2)]\n",
    "        for i in iter_num:\n",
    "            filter_by_cutoff(i, valid_data)\n",
    "\n",
    "        index_all = list(pd.Series(valid_data.index).apply(lambda x: datetime.strftime(x, \"%Y-%m\")).unique())\n",
    "        all_valid_summmary = pd.DataFrame(columns=['precision_score', 'recall_score', 'f1_score', 'date_y_m'])\n",
    "\n",
    "        for i in index_all:\n",
    "            temp_valid_data = valid_data[i]\n",
    "            temp_valid_summary = summary_score(temp_valid_data)\n",
    "            date_y_m_value = i\n",
    "            temp_valid_summary['date_y_m'] = date_y_m_value\n",
    "            all_valid_summmary = all_valid_summmary.append(temp_valid_summary)\n",
    "\n",
    "        all_valid_summmary['cutoff'] = all_valid_summmary.index\n",
    "        all_valid_summmary = all_valid_summmary.reset_index(drop=True)\n",
    "\n",
    "        return (all_valid_summmary)\n",
    "\n",
    "    def final_summary(test_result):\n",
    "\n",
    "        mdate = list(test_result.index.unique().strftime(\"%Y-%m\"))[0]\n",
    "        REAL = test_result['REAL']\n",
    "        predict = test_result['predict_final']\n",
    "        p_s = precision_score(REAL, predict)\n",
    "        r_s = recall_score(REAL, predict)\n",
    "        f_s = f1_score(REAL, predict)\n",
    "\n",
    "        distress_num = test_result['REAL'].sum()\n",
    "        predict_num = test_result['predict_final'].sum()\n",
    "        REAL_AND_predict_num = sum((test_result['REAL'] + test_result['predict_final']) == 2)\n",
    "        final_result = pd.DataFrame([[mdate, distress_num, predict_num, REAL_AND_predict_num, p_s, r_s, f_s]],\n",
    "                                    columns=['mdate', 'distress_num', 'predict_num', 'REAL_AND_predict_num',\n",
    "                                             'precision_score', 'recall_score', 'f1_score'])\n",
    "        return (final_result)\n",
    "\n",
    "    # Grid Search parameter setting\n",
    "    from itertools import product\n",
    "    parameter_set = [[5, 10, 20, 35, 50], ['liblinear', 'newton-cg', 'saga', 'sag'],\n",
    "                     ['minmaxscaler', 'standardscaler', 'maxabsscaler',\n",
    "                      'robustscaler']]\n",
    "    parameter_set_frame = pd.DataFrame(list(product(*parameter_set)), columns=['num_iter', 'solver_type',\n",
    "                                                                               'scaler_type'])\n",
    "\n",
    "    # Final Output Columns\n",
    "    result_final_all = pd.DataFrame(columns=['mdate', 'distress_num', 'predict_num', 'REAL_AND_predict_num',\n",
    "                                             'precision_score', 'recall_score', 'f1_score', 'cutoff',\n",
    "                                             'num_iter', 'solver_type', 'scaler_type'])\n",
    "\n",
    "# Data loading & prepping\n",
    "    #Set exch_type_list for actual test\n",
    "    if backtest_flag == False:\n",
    "        exch_type_list = ['KOSPI','KOSDAQ']\n",
    "    else: exch_type_list = [exch_type]\n",
    "    result_dict = {}\n",
    "    \n",
    "    for exch_name in exch_type_list:\n",
    "        print(f'Processing {exch_name.upper()}')\n",
    "        exch_type = exch_name\n",
    "        temp = read_file()\n",
    "        data_all = factor_prep(temp)\n",
    "        if backtest_flag == True:\n",
    "            train_begin = backtest_start_date\n",
    "            end_date = datetime.strftime(datetime.strptime(backtest_end_date, '%Y-%m-%d'), '%Y-%m')\n",
    "        elif backtest_flag == False:\n",
    "            train_begin = datetime.strftime(\n",
    "                datetime.strptime(target_date, '%Y-%m-%d') - relativedelta(months=+73) - relativedelta(days=+1), '%Y-%m-%d')\n",
    "            end_date = datetime.strftime(datetime.strptime(target_date, '%Y-%m-%d'), '%Y-%m')\n",
    "        # Rolling windows by shifting date\n",
    "        while True:\n",
    "\n",
    "            train_end = datetime.strptime(train_begin, '%Y-%m-%d') + relativedelta(years=+5)\n",
    "            valid_begin = train_end + relativedelta(days=+1)\n",
    "            valid_end = valid_begin + relativedelta(years=+1)\n",
    "            test_date = valid_end + relativedelta(months=+1)\n",
    "            test_date = datetime.strftime(test_date, '%Y-%m')\n",
    "\n",
    "            train_data = data_all.loc[train_begin: datetime.strftime(train_end, '%Y-%m-%d')]\n",
    "            valid_data = data_all.loc[datetime.strftime(valid_begin, '%Y-%m-%d'): datetime.strftime(valid_end, '%Y-%m-%d')]\n",
    "            test_data = data_all.loc[test_date]\n",
    "\n",
    "            temp_data_result = pd.DataFrame(columns=['precision_score', 'recall_score', 'f1_score',\n",
    "                                                     'date_y_m', 'num_iter', 'solver_type', 'scaler_type'])\n",
    "\n",
    "            # Finding best parameters in validation set\n",
    "            for i in tqdm_notebook(range(len(parameter_set_frame.index)),\n",
    "                                   desc=f'Test_Date: {test_date}'):\n",
    "                valid_predict = dynamic_logistic(train_data, valid_data,\n",
    "                                                 parameter_set_frame['num_iter'][i],\n",
    "                                                 parameter_set_frame['solver_type'][i],\n",
    "                                                 parameter_set_frame['scaler_type'][i])\n",
    "\n",
    "                valid_predict_result = validate_cutoff(valid_predict)\n",
    "                valid_predict_result['num_iter'] = parameter_set_frame['num_iter'][i]\n",
    "                valid_predict_result['solver_type'] = parameter_set_frame['solver_type'][i]\n",
    "                valid_predict_result['scaler_type'] = parameter_set_frame['scaler_type'][i]\n",
    "                temp_data_result = temp_data_result.append(valid_predict_result)\n",
    "                sleep(0.1)\n",
    "\n",
    "            best_valid_parameter = pd.DataFrame(\n",
    "                temp_data_result.groupby(['cutoff', 'num_iter', 'solver_type', 'scaler_type']).mean()[\n",
    "                    'f1_score'].idxmax()).T\n",
    "            best_valid_parameter.columns = ['cutoff', 'num_iter', 'solver_type', 'scaler_type']\n",
    "\n",
    "            test_result = dynamic_logistic(train_data, test_data,\n",
    "                                           best_valid_parameter['num_iter'][0],\n",
    "                                           best_valid_parameter['solver_type'][0],\n",
    "                                           best_valid_parameter['scaler_type'][0])\n",
    "\n",
    "            # Adding Predict_final column and Setting Cutoff parameter with \"best_valid_parameter\"\n",
    "            test_result['predict_final'] = 0\n",
    "            test_result['predict_final'] = test_result['predict'].apply(lambda x: 1\n",
    "            if x > float(best_valid_parameter['cutoff']) else 0)\n",
    "\n",
    "            if backtest_flag == True:\n",
    "\n",
    "                test_result_summary = final_summary(test_result)\n",
    "                test_result_summary['cutoff'] = float(best_valid_parameter['cutoff'])\n",
    "                test_result_summary['num_iter'] = best_valid_parameter['num_iter']\n",
    "                test_result_summary['solver_type'] = best_valid_parameter['solver_type']\n",
    "                test_result_summary['scaler_type'] = best_valid_parameter['scaler_type']\n",
    "\n",
    "                test_result_summary['train_begin'] = train_begin\n",
    "                test_result_summary['train_end'] = train_end\n",
    "                test_result_summary['valid_begin'] = valid_begin\n",
    "                test_result_summary['valid_end'] = valid_end\n",
    "                test_result_summary['test_date'] = test_date\n",
    "                print(test_result_summary)\n",
    "\n",
    "                result_final_all = result_final_all.append(test_result_summary)\n",
    "                result_final_all = result_final_all[['test_date', 'train_begin', 'train_end', 'valid_begin', 'valid_end',\n",
    "                                                     'distress_num', 'predict_num', 'REAL_AND_predict_num',\n",
    "                                                     'precision_score',\n",
    "                                                     'recall_score', 'f1_score', 'cutoff', 'num_iter', 'solver_type',\n",
    "                                                     'scaler_type']]\n",
    "\n",
    "            elif backtest_flag == False:\n",
    "                print('cutoff:', float(best_valid_parameter['cutoff'][0]))\n",
    "                print('num_iter:', best_valid_parameter['num_iter'][0])\n",
    "                print('solver_type:', best_valid_parameter['solver_type'][0])\n",
    "                print('scaler_type:', best_valid_parameter['scaler_type'][0],'\\n','\\n')\n",
    "                result_final_all = test_result[['code', 'predict', 'predict_final']]\n",
    "                break\n",
    "                \n",
    "            if test_date == end_date:\n",
    "                break\n",
    "\n",
    "            train_begin = datetime.strptime(train_begin, '%Y-%m-%d') + relativedelta(months=+1)\n",
    "            train_begin = datetime.strftime(train_begin, '%Y-%m-%d')\n",
    "\n",
    "        result_dict[exch_name] = result_final_all\n",
    "        \n",
    "    if backtest_flag == False:\n",
    "        final_result = pd.concat([result_dict['KOSPI'],result_dict['KOSDAQ']],axis=0)\n",
    "        final_result.columns = ['code', 'distress_score', 'distress_indicator']\n",
    "        final_result = pd.merge(left=read_file(), right=final_result, how='left', on=['code','mdate'], sort=False)\n",
    "    else:\n",
    "        final_result = result_dict[exch_name]\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing KOSPI\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02d795fcbb44367a23311940ac1dfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test_Date: 2020-04', max=80.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cutoff: 0.66\n",
      "num_iter: 50\n",
      "solver_type: liblinear\n",
      "scaler_type: standardscaler \n",
      " \n",
      "\n",
      "Processing KOSDAQ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6846e89b7c41bea1a2b452ee80cee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test_Date: 2020-04', max=80.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cutoff: 0.7\n",
      "num_iter: 10\n",
      "solver_type: saga\n",
      "scaler_type: robustscaler \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = distress_detector('./Data/2020-04-30_company.pck', backtest_flag=False, target_date = '2020-04-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.to_pickle('./2020-04-30_distress_test_result.pck')\n",
    "history.to_csv('./2020-04-30_distress_test_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.to_excel('2020-04-30_distress_test_result.xlsx', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
